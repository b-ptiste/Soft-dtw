{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw_soft import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dtw(\n",
    "    x: torch.Tensor, y: torch.Tensor, gamma: float = 1.0\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Soft Dynamic Time Warping.\n",
    "\n",
    "    Args:\n",
    "        x (list): length x feature\n",
    "        y (list): length x feature\n",
    "        gamma (float, optional): gamma parameter. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        float: soft-DTW distance\n",
    "    \"\"\"\n",
    "    # initialize DP matrix\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    R = torch.zeros((n + 1, m + 1))\n",
    "    R[0, 1:] = float(\"inf\")\n",
    "    R[1:, 0] = float(\"inf\")\n",
    "    R[0, 0] = 0.0\n",
    "\n",
    "    try:\n",
    "        cost = torch.cdist(x, y, p=2) ** 2\n",
    "    except:\n",
    "        print(\n",
    "            \"Carefull : x and y are not D-dimensional > 1 features : added 2 dimensions\"\n",
    "        )\n",
    "        cost = torch.cdist(x.unsqueeze(1), y.unsqueeze(1), p=2) ** 2\n",
    "\n",
    "    for j in range(1, m + 1):\n",
    "        for i in range(1, n + 1):\n",
    "            # calculate minimum\n",
    "            _min = soft_min([R[i - 1, j], R[i, j - 1], R[i - 1, j - 1]], gamma)\n",
    "\n",
    "            # update cell\n",
    "            R[i, j] = cost[i - 1, j - 1] + _min\n",
    "\n",
    "    return R[-1, -1], R, cost\n",
    "\n",
    "\n",
    "def jacobian_product_sq_euc_optimized(X, Y, E):\n",
    "    # Expand X and Y to 3D tensors for broadcasting\n",
    "    X_expanded = X.unsqueeze(1)  # Shape: [m, 1, d]\n",
    "    Y_expanded = Y.unsqueeze(0)  # Shape: [1, n, d]\n",
    "\n",
    "    # Compute the squared differences, shape: [m, n, d]\n",
    "    diff = X_expanded - Y_expanded\n",
    "\n",
    "    # Compute the weighted differences, shape: [m, n, d]\n",
    "    weighted_diff = E.unsqueeze(-1) * diff * 2\n",
    "\n",
    "    # Sum over the second dimension (n) to get the result, shape: [m, d]\n",
    "    G = weighted_diff.sum(dim=1)\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "def backward_recursion(\n",
    "    x: torch.Tensor, y: torch.Tensor, R, delta, gamma: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"backward recursion of soft-DTW\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): length x feature\n",
    "        y (torch.Tensor): length x feature\n",
    "        gamma (float, optional): gamma parameter. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: E matrix\n",
    "    \"\"\"\n",
    "    n, m = x.shape[0], y.shape[0]\n",
    "\n",
    "    # intialization\n",
    "    # compute R\n",
    "    # _, R, delta = soft_dtw(x, y, gamma=gamma)\n",
    "    R = torch.cat((R, -float(\"inf\") * torch.ones((n + 1)).reshape(-1, 1)), dim=1)\n",
    "    R = torch.cat((R, -float(\"inf\") * torch.ones((m + 2)).reshape(1, -1)), dim=0)\n",
    "    R[n + 1, m + 1] = R[n, m]\n",
    "\n",
    "    # delta[:-1, m], delta[n, :-1] = 0.0, 0.0\n",
    "    delta = torch.cat((delta, torch.zeros((n)).reshape(-1, 1)), dim=1)\n",
    "    delta = torch.cat((delta, torch.zeros((m + 1)).reshape(1, -1)), dim=0)\n",
    "    delta[n, m] = 0.0\n",
    "\n",
    "    # compute E\n",
    "    E = torch.zeros((n + 2, m + 2))\n",
    "    E[n + 1, m + 1] = 1.0\n",
    "\n",
    "\n",
    "\n",
    "    # backward recursion\n",
    "    for j in range(m, 0, -1):  # ranges from m to 1\n",
    "        for i in range(n, 0, -1):  # ranges from n to 1\n",
    "            a = torch.exp((R[i + 1, j] - R[i, j] - delta[i, j - 1]) / gamma)\n",
    "            b = torch.exp((R[i, j + 1] - R[i, j] - delta[i - 1, j]) / gamma)\n",
    "            c = torch.exp((R[i + 1, j + 1] - R[i, j] - delta[i, j]) / gamma)\n",
    "            E[i, j] = E[i + 1, j] * a + E[i, j + 1] * b + E[i + 1, j + 1] * c\n",
    "\n",
    "    return E[1:-1, 1:-1]\n",
    "\n",
    "class SoftDTWFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, target, gamma):\n",
    "        loss, R, delta = soft_dtw(input, target, gamma)\n",
    "\n",
    "        # save data for backward\n",
    "        ctx.save_for_backward(input, target, R, delta)\n",
    "        ctx.gamma = gamma\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # get value from forward\n",
    "        x, y, R, delta = ctx.saved_tensors\n",
    "        E = backward_recursion(x, y, R, delta, ctx.gamma)\n",
    "        q = jacobian_product_sq_euc_optimized(x, y, E)\n",
    "        return q, None, None\n",
    "\n",
    "\n",
    "class MyDtw(torch.nn.Module):\n",
    "    def __init__(self, gamma=1):\n",
    "        super(MyDtw, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Use self.param in your loss computation\n",
    "        loss = SoftDTWFunction.apply(input, target, self.gamma)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 : torch.Size([3, 2]) y1 : torch.Size([5, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3186.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor([[1, 1, 56], [2, 8, 0]], dtype=torch.float32,requires_grad=True).T\n",
    "y1 = torch.tensor([[1, 8, 1, 9, 1], [5, 9, 14, 7, -1]], dtype=torch.float32,requires_grad=True).T\n",
    "\n",
    "x1.retain_grad()\n",
    "y1.retain_grad()\n",
    "print(\"x1 :\",x1.shape,\"y1 :\",y1.shape)\n",
    "\n",
    "cost, R, dist = soft_dtw(x1, y1, gamma=1.0)\n",
    "cost.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MyDtw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3186., grad_fn=<SoftDTWFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_custom = criterion(x1,y1)\n",
    "print(loss_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([3, 2])\n",
      "tensor([[-1.9950e-20, -6.0000e+00],\n",
      "        [-3.0000e+01, -1.1999e+01],\n",
      "        [ 1.1000e+02,  2.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "loss_custom.backward()\n",
    "print(\"x\",x1.shape)\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = torch.tensor([2.0,5.0,3.0,6.0],requires_grad=True).unsqueeze(-1)\n",
    "#y = torch.tensor([5.0,9.0,2.0],requires_grad=True).unsqueeze(-1)\n",
    "x2 = torch.tensor([[1, 1, 56], [2, 8, 0]], dtype=torch.float32,requires_grad=True).T\n",
    "y2 = torch.tensor([[1, 8, 1, 9, 1], [5, 9, 14, 7, -1]], dtype=torch.float32,requires_grad=True).T\n",
    "\n",
    "x2.retain_grad()\n",
    "y2.retain_grad()\n",
    "\n",
    "loss = soft_dtw(x2,y2)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([3, 2])\n",
      "tensor([[  0.,   0.],\n",
      "        [  0.,   0.],\n",
      "        [110.,   2.]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(\"x\",x2.shape)\n",
    "print(x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_min_batch(list_a, gamma):\n",
    "    \"\"\"Softmin function.\n",
    "\n",
    "    Args:\n",
    "        list_a (list): list of values\n",
    "        gamma (float): gamma parameter\n",
    "\n",
    "    Returns:\n",
    "        float: softmin value\n",
    "    \"\"\"\n",
    "    assert gamma >= 0, \"gamma must be greater than or equal to 0\"\n",
    "    # Assuming list_a is a list of tensors of the same shape\n",
    "    list_a = torch.stack(list_a)  # Shape: [n, m]\n",
    "\n",
    "    if gamma == 0:\n",
    "        _min = torch.min(list_a, dim=0)[0]  # Min along the first dimension\n",
    "    else:\n",
    "        z = -list_a / gamma\n",
    "        max_z = torch.max(z, dim=0, keepdim=True)[0]  # Max along the first dimension\n",
    "        log_sum = max_z + torch.log(torch.sum(torch.exp(z - max_z), dim=0))\n",
    "        _min = -gamma * log_sum\n",
    "    return _min\n",
    "\n",
    "def soft_dtw_batch_same_size(x: torch.Tensor, y: torch.Tensor, gamma: float = 1) -> torch.Tensor:\n",
    "    \"\"\"Soft Dynamic Time Warping.\n",
    "\n",
    "    Args:\n",
    "        x (list): batch x length x feature\n",
    "        y (list): batch x length x feature\n",
    "        gamma (float, optional): gamma parameter. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        float: soft-DTW distance\n",
    "    \"\"\"\n",
    "    # initialize DP matrix\n",
    "    n = x.shape[1]\n",
    "    m = y.shape[1]\n",
    "    b = x.shape[0]\n",
    "\n",
    "    R = torch.zeros((b, n + 1, m + 1))\n",
    "    R[:, 0, 1:] = float(\"inf\")\n",
    "    R[:, 1:, 0] = float(\"inf\")\n",
    "    R[:, 0, 0] = 0.0\n",
    "\n",
    "    try:\n",
    "        cost = torch.cdist(x, y, p=2) ** 2\n",
    "    except:\n",
    "        print(\n",
    "            \"Carefull : x and y are not D-dimensional > 1 features : added 2 dimensions\"\n",
    "        )\n",
    "        cost = torch.cdist(x.unsqueeze(1), y.unsqueeze(1), p=2) ** 2\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            R[:, i, j] = cost[:, i - 1, j - 1] + soft_min_batch([R[:, i - 1, j], R[:, i, j - 1], R[:, i - 1, j - 1]], gamma)\n",
    "\n",
    "    return R[:, -1, -1], R, cost\n",
    "\n",
    "\n",
    "def backward_recursion_batch_same_size(\n",
    "    x: torch.Tensor, y: torch.Tensor, R, delta, gamma: float = 1.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"backward recursion of soft-DTW\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): batch x length x feature\n",
    "        y (torch.Tensor): batch x length x feature\n",
    "        gamma (float, optional): gamma parameter. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: E batch x matrix\n",
    "    \"\"\"\n",
    "    batch = x.shape[0]\n",
    "    n, m = x.shape[1], y.shape[1]\n",
    "\n",
    "    # intialization\n",
    "    delta = torch.cat((delta, torch.zeros((batch,n)).reshape(batch, -1, 1)), dim=2)\n",
    "    delta = torch.cat((delta, torch.zeros((batch,m + 1)).reshape(batch, 1, -1)), dim=1)\n",
    "    delta[:, n, m] = 0.0\n",
    "\n",
    "    # compute E\n",
    "    E = torch.zeros((batch, n + 2, m + 2))\n",
    "    E[:, n + 1, m + 1] = 1.0\n",
    "\n",
    "    # compute R\n",
    "    # _, R = soft_dtw_batch_same_size(x, y, gamma=gamma)\n",
    "    R = torch.cat((R, -float(\"inf\") * torch.ones((batch,n + 1)).reshape(batch, -1, 1)), dim=2)\n",
    "    R = torch.cat((R, -float(\"inf\") * torch.ones((batch,m + 2)).reshape(batch, 1, -1)), dim=1)\n",
    "    R[:, n + 1, m + 1] = R[:, n, m]\n",
    "\n",
    "    # backward recursion\n",
    "    for j in range(m, 0, -1):  # ranges from m to 1\n",
    "        for i in range(n, 0, -1):  # ranges from n to 1\n",
    "            a = torch.exp((R[:, i + 1, j] - R[:, i, j] - delta[:, i, j - 1]) / gamma)\n",
    "            b = torch.exp((R[:, i, j + 1] - R[:, i, j] - delta[:, i - 1, j]) / gamma)\n",
    "            c = torch.exp((R[:, i + 1, j + 1] - R[:, i, j] - delta[:, i, j]) / gamma)\n",
    "            E[:, i, j] = E[:, i + 1, j] * a + E[:, i, j + 1] * b + E[:, i + 1, j + 1] * c\n",
    "\n",
    "    return E[:, 1:-1, 1:-1]\n",
    "\n",
    "def jacobian_product_sq_euc_batch(X, Y, E):\n",
    "    # Expand X and Y to 4D tensors for broadcasting, shape: [b, m, 1, d] and [b, 1, n, d]\n",
    "    X_expanded = X.unsqueeze(2)\n",
    "    Y_expanded = Y.unsqueeze(1)\n",
    "\n",
    "    # Compute the squared differences, shape: [b, m, n, d]\n",
    "    diff = X_expanded - Y_expanded\n",
    "\n",
    "    # Adjust E for broadcasting, shape: [b, m, n, d]\n",
    "    E_adjusted = E.unsqueeze(-1)\n",
    "\n",
    "    # Compute the weighted differences, shape: [b, m, n, d]\n",
    "    weighted_diff = E_adjusted * diff * 2\n",
    "\n",
    "    # Sum over the third dimension (n) to get the result, shape: [b, m, d]\n",
    "    G = weighted_diff.sum(dim=2)\n",
    "    return G\n",
    "\n",
    "class SoftDTWFunction_batch_same_size(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, target, gamma):\n",
    "        loss, R, delta = soft_dtw_batch_same_size(input, target, gamma)\n",
    "\n",
    "        # save data for backward\n",
    "        ctx.save_for_backward(input, target, R, delta)\n",
    "        ctx.gamma = gamma\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # get value from forward\n",
    "        x, y, R, delta = ctx.saved_tensors\n",
    "        E = backward_recursion_batch_same_size(x, y, R, delta, ctx.gamma)\n",
    "        q = jacobian_product_sq_euc_batch(x, y, E)\n",
    "        return q/x.shape[0], None, None\n",
    "\n",
    "\n",
    "class DTWLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=1,reduction='mean'):\n",
    "        super(DTWLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Use self.param in your loss computation\n",
    "        if self.reduction == 'mean':\n",
    "            loss =torch.mean(SoftDTWFunction_batch_same_size.apply(input, target, self.gamma))\n",
    "        elif self.reduction == 'sum':\n",
    "            loss =torch.sum(SoftDTWFunction_batch_same_size.apply(input, target, self.gamma))\n",
    "        else:\n",
    "            raise \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 : torch.Size([2, 3, 2]) y1 : torch.Size([2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "xs1_1 = torch.tensor([[1, 1, 56], [2, 8, 0]], dtype=torch.float32,requires_grad=True).T#.repeat(1,1,1)\n",
    "ys1_1 = torch.tensor([[1, 8, 1, 9, 1], [5, 9, 14, 7, -1]], dtype=torch.float32,requires_grad=True).T#.repeat(1,1,1)\n",
    "\n",
    "xs1_2 = torch.tensor([[1, 0, 56], [2, 0, 0]], dtype=torch.float32,requires_grad=True).T#.repeat(1,1,1)\n",
    "ys1_2 = torch.tensor([[1, 0, 1, 9, 1], [5, 0, 14, 7, -1]], dtype=torch.float32,requires_grad=True).T#.repeat(1,1,1)\n",
    "\n",
    "xs1 = torch.stack([xs1_1,xs1_2])\n",
    "ys1 = torch.stack([ys1_1,ys1_2])\n",
    "\n",
    "xs1.retain_grad()\n",
    "ys1.retain_grad()\n",
    "print(\"x1 :\",xs1.shape,\"y1 :\",ys1.shape)\n",
    "\n",
    "cost, R, dist = soft_dtw_batch_same_size(xs1, ys1, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DTWLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3230.5000, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_custom = criterion(xs1,ys1)\n",
    "loss_custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([2, 3, 2])\n",
      "tensor([[[-9.9749e-21, -3.0000e+00],\n",
      "         [-1.5000e+01, -5.9996e+00],\n",
      "         [ 5.5000e+01,  1.0000e+00]],\n",
      "\n",
      "        [[-7.0000e+00, -1.8000e+01],\n",
      "         [-1.0000e+00,  1.0000e+00],\n",
      "         [ 5.5000e+01,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "loss_custom.backward()\n",
    "print(\"x\",xs1.shape)\n",
    "print(xs1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 : torch.Size([2, 3, 2]) y1 : torch.Size([2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "xs2_1 = torch.tensor([[1, 1, 56], [2, 8, 0]], dtype=torch.float32,requires_grad=True).T#.repeat(1,1,1)\n",
    "ys2_1 = torch.tensor([[1, 8, 1, 9, 1], [5, 9, 14, 7, -1]], dtype=torch.float32,requires_grad=True).T#.repeat(1,1,1)\n",
    "\n",
    "xs2_2 = torch.tensor([[1, 0, 56], [2, 0, 0]], dtype=torch.float32,requires_grad=True).T#.repeat(1,1,1)\n",
    "ys2_2 = torch.tensor([[1, 0, 1, 9, 1], [5, 0, 14, 7, -1]], dtype=torch.float32,requires_grad=True).T#.repeat(1,1,1)\n",
    "\n",
    "xs2 = torch.stack([xs1_1,xs1_2])\n",
    "ys2 = torch.stack([ys1_1,ys1_2])\n",
    "\n",
    "xs2.retain_grad()\n",
    "ys2.retain_grad()\n",
    "print(\"x1 :\",xs2.shape,\"y1 :\",ys2.shape)\n",
    "\n",
    "loss = torch.mean(soft_dtw_batch_same_size(xs2,ys2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([2, 3, 2])\n",
      "tensor([[[-9.9749e-21, -3.0000e+00],\n",
      "         [-1.5000e+01, -5.9996e+00],\n",
      "         [ 5.5000e+01,  1.0000e+00]],\n",
      "\n",
      "        [[-7.0000e+00, -1.8000e+01],\n",
      "         [-1.0000e+00,  1.0000e+00],\n",
      "         [ 5.5000e+01,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(\"x\",xs2.shape)\n",
    "print(xs2.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MVA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
